{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9ecce3a-a7d5-4756-94dc-8b0c15c126d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Enter your token (input will not be visible): Traceback (most recent call last):\n",
      "  File \"/home/pengejeen/.local/bin/huggingface-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/pengejeen/.local/lib/python3.10/site-packages/huggingface_hub/commands/huggingface_cli.py\", line 51, in main\n",
      "    service.run()\n",
      "  File \"/home/pengejeen/.local/lib/python3.10/site-packages/huggingface_hub/commands/user.py\", line 98, in run\n",
      "    login(token=self.args.token, add_to_git_credential=self.args.add_to_git_credential)\n",
      "  File \"/home/pengejeen/.local/lib/python3.10/site-packages/huggingface_hub/_login.py\", line 115, in login\n",
      "    interpreter_login(new_session=new_session, write_permission=write_permission)\n",
      "  File \"/home/pengejeen/.local/lib/python3.10/site-packages/huggingface_hub/_login.py\", line 191, in interpreter_login\n",
      "    token = getpass(\"Enter your token (input will not be visible): \")\n",
      "  File \"/usr/lib/python3.10/getpass.py\", line 77, in unix_getpass\n",
      "    passwd = _raw_input(prompt, stream, input=input)\n",
      "  File \"/usr/lib/python3.10/getpass.py\", line 146, in _raw_input\n",
      "    line = input.readline()\n",
      "  File \"/usr/lib/python3.10/codecs.py\", line 319, in decode\n",
      "    def decode(self, input, final=False):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d063658f-0326-4bdd-82d7-5035bfe112fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b7f50beaec2450da79fcbeb8968d635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da674a6f027d477ab9e0829c8e479ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 done\n",
      "2 done\n",
      "3 done\n",
      "4 done\n",
      "5 done\n",
      "6 done\n",
      "7 done\n",
      "8 done\n",
      "9 done\n",
      "10 done\n",
      "11 done\n",
      "12 done\n",
      "13 done\n",
      "14 done\n",
      "15 done\n",
      "16 done\n",
      "17 done\n",
      "18 done\n",
      "19 done\n",
      "20 done\n",
      "21 done\n",
      "22 done\n",
      "23 done\n",
      "24 done\n",
      "25 done\n",
      "26 done\n",
      "27 done\n",
      "28 done\n",
      "29 done\n",
      "30 done\n",
      "31 done\n",
      "32 done\n",
      "33 done\n",
      "34 done\n",
      "35 done\n",
      "36 done\n",
      "37 done\n",
      "38 done\n",
      "39 done\n",
      "40 done\n",
      "41 done\n",
      "42 done\n",
      "43 done\n",
      "44 done\n",
      "45 done\n",
      "46 done\n",
      "47 done\n",
      "48 done\n",
      "49 done\n",
      "50 done\n",
      "51 done\n",
      "52 done\n",
      "53 done\n",
      "54 done\n",
      "55 done\n",
      "56 done\n",
      "57 done\n",
      "58 done\n",
      "59 done\n",
      "60 done\n",
      "61 done\n",
      "62 done\n",
      "63 done\n",
      "64 done\n",
      "65 done\n",
      "66 done\n",
      "67 done\n",
      "68 done\n",
      "69 done\n",
      "70 done\n",
      "71 done\n",
      "72 done\n",
      "73 done\n",
      "74 done\n",
      "75 done\n",
      "76 done\n",
      "77 done\n",
      "78 done\n",
      "79 done\n",
      "80 done\n",
      "81 done\n",
      "82 done\n",
      "83 done\n",
      "84 done\n",
      "85 done\n",
      "86 done\n",
      "87 done\n",
      "88 done\n",
      "89 done\n",
      "90 done\n",
      "91 done\n",
      "92 done\n",
      "93 done\n",
      "94 done\n",
      "95 done\n",
      "96 done\n",
      "97 done\n",
      "98 done\n",
      "99 done\n",
      "100 done\n",
      "101 done\n",
      "102 done\n",
      "103 done\n",
      "104 done\n",
      "105 done\n",
      "106 done\n",
      "107 done\n",
      "108 done\n",
      "109 done\n",
      "110 done\n",
      "111 done\n",
      "112 done\n",
      "113 done\n",
      "114 done\n",
      "115 done\n",
      "116 done\n",
      "117 done\n",
      "118 done\n",
      "119 done\n",
      "120 done\n",
      "121 done\n",
      "122 done\n",
      "123 done\n",
      "124 done\n",
      "125 done\n",
      "126 done\n",
      "127 done\n",
      "128 done\n",
      "129 done\n",
      "130 done\n",
      "131 done\n",
      "132 done\n",
      "133 done\n",
      "134 done\n",
      "135 done\n",
      "136 done\n",
      "137 done\n",
      "138 done\n",
      "139 done\n",
      "140 done\n",
      "141 done\n",
      "142 done\n",
      "143 done\n",
      "144 done\n",
      "145 done\n",
      "146 done\n",
      "147 done\n",
      "148 done\n",
      "149 done\n",
      "150 done\n",
      "151 done\n",
      "152 done\n",
      "153 done\n",
      "154 done\n",
      "155 done\n",
      "156 done\n",
      "157 done\n",
      "158 done\n",
      "159 done\n",
      "160 done\n",
      "161 done\n",
      "162 done\n",
      "163 done\n",
      "164 done\n",
      "165 done\n",
      "166 done\n",
      "167 done\n",
      "168 done\n",
      "169 done\n",
      "170 done\n",
      "171 done\n",
      "172 done\n",
      "173 done\n",
      "174 done\n",
      "175 done\n",
      "176 done\n",
      "177 done\n",
      "178 done\n",
      "179 done\n",
      "180 done\n",
      "181 done\n",
      "182 done\n",
      "183 done\n",
      "184 done\n",
      "185 done\n",
      "186 done\n",
      "187 done\n",
      "188 done\n",
      "189 done\n",
      "190 done\n",
      "191 done\n",
      "192 done\n",
      "193 done\n",
      "194 done\n",
      "195 done\n",
      "196 done\n",
      "197 done\n",
      "198 done\n",
      "199 done\n",
      "200 done\n",
      "201 done\n",
      "202 done\n",
      "203 done\n",
      "204 done\n",
      "205 done\n",
      "206 done\n",
      "207 done\n",
      "208 done\n",
      "209 done\n",
      "210 done\n",
      "211 done\n",
      "212 done\n",
      "213 done\n",
      "214 done\n",
      "215 done\n",
      "216 done\n",
      "217 done\n",
      "218 done\n",
      "219 done\n",
      "220 done\n",
      "221 done\n",
      "222 done\n",
      "223 done\n",
      "224 done\n",
      "225 done\n",
      "226 done\n",
      "227 done\n",
      "228 done\n",
      "229 done\n",
      "230 done\n",
      "231 done\n",
      "232 done\n",
      "233 done\n",
      "234 done\n",
      "235 done\n",
      "236 done\n",
      "237 done\n",
      "238 done\n",
      "239 done\n",
      "240 done\n",
      "241 done\n",
      "242 done\n",
      "243 done\n",
      "244 done\n",
      "245 done\n",
      "246 done\n",
      "247 done\n",
      "248 done\n",
      "249 done\n",
      "250 done\n",
      "251 done\n",
      "252 done\n",
      "253 done\n",
      "254 done\n",
      "255 done\n",
      "256 done\n",
      "257 done\n",
      "258 done\n",
      "259 done\n",
      "260 done\n",
      "261 done\n",
      "262 done\n",
      "263 done\n",
      "264 done\n",
      "265 done\n",
      "266 done\n",
      "267 done\n",
      "268 done\n",
      "269 done\n",
      "270 done\n",
      "271 done\n",
      "272 done\n",
      "273 done\n",
      "274 done\n",
      "275 done\n",
      "276 done\n",
      "277 done\n",
      "278 done\n",
      "279 done\n",
      "280 done\n",
      "281 done\n",
      "282 done\n",
      "283 done\n",
      "284 done\n",
      "285 done\n",
      "286 done\n",
      "287 done\n",
      "288 done\n",
      "289 done\n",
      "290 done\n",
      "291 done\n",
      "292 done\n",
      "293 done\n",
      "294 done\n",
      "295 done\n",
      "296 done\n",
      "297 done\n",
      "298 done\n",
      "299 done\n",
      "300 done\n",
      "301 done\n",
      "302 done\n",
      "303 done\n",
      "304 done\n",
      "305 done\n",
      "306 done\n",
      "307 done\n",
      "308 done\n",
      "309 done\n",
      "310 done\n",
      "311 done\n",
      "312 done\n",
      "313 done\n",
      "314 done\n",
      "315 done\n",
      "316 done\n",
      "317 done\n",
      "318 done\n",
      "319 done\n",
      "320 done\n",
      "321 done\n",
      "322 done\n",
      "323 done\n",
      "324 done\n",
      "325 done\n",
      "326 done\n",
      "327 done\n",
      "328 done\n",
      "329 done\n",
      "330 done\n",
      "331 done\n",
      "332 done\n",
      "333 done\n",
      "334 done\n",
      "335 done\n",
      "336 done\n",
      "337 done\n",
      "338 done\n",
      "339 done\n",
      "340 done\n",
      "341 done\n",
      "342 done\n",
      "343 done\n",
      "344 done\n",
      "345 done\n",
      "346 done\n",
      "347 done\n",
      "348 done\n",
      "349 done\n",
      "350 done\n",
      "351 done\n",
      "352 done\n",
      "353 done\n",
      "354 done\n",
      "355 done\n",
      "356 done\n",
      "357 done\n",
      "358 done\n",
      "359 done\n",
      "360 done\n",
      "361 done\n",
      "362 done\n",
      "363 done\n",
      "364 done\n",
      "365 done\n",
      "366 done\n",
      "367 done\n",
      "368 done\n",
      "369 done\n",
      "370 done\n",
      "371 done\n",
      "372 done\n",
      "373 done\n",
      "374 done\n",
      "375 done\n",
      "376 done\n",
      "377 done\n",
      "378 done\n",
      "379 done\n",
      "380 done\n",
      "381 done\n",
      "382 done\n",
      "383 done\n",
      "384 done\n",
      "385 done\n",
      "386 done\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "import os\n",
    "\n",
    "# 스트리밍 옵션으로 데이터셋 로드\n",
    "dataset = load_dataset('simon3000/genshin-voice', split='train', streaming=True)\n",
    "\n",
    "# 한국어 Paimon 음성과 전사(transcription)가 있는 데이터를 필터링하는 함수 정의\n",
    "def filter_korean_paimon(example):\n",
    "    return example['language'] == 'Korean' and example['speaker'] == 'Paimon' and example['transcription'] != ''\n",
    "\n",
    "# 폴더 생성\n",
    "paimon_folder = 'paimon_voices'\n",
    "os.makedirs(paimon_folder, exist_ok=True)\n",
    "# 데이터셋의 일부분만 처리\n",
    "batch_size = 100  # 한 번에 처리할 데이터 개수\n",
    "i = 1\n",
    "\n",
    "# 데이터 처리 루프\n",
    "start_index = 0  # 시작 인덱스 설정\n",
    "while True:\n",
    "    # 데이터셋에서 시작 인덱스부터 배치를 가져옴\n",
    "    batch = list(islice(dataset, start_index, start_index + batch_size))\n",
    "    if not batch:\n",
    "        break\n",
    "\n",
    "    for example in batch:\n",
    "        # 필터링 조건 확인\n",
    "        if filter_korean_paimon(example):\n",
    "            audio_path = os.path.join(paimon_folder, f'{i}_audio.wav')  # 오디오 파일 저장 경로\n",
    "            transcription_path = os.path.join(paimon_folder, f'{i}_transcription.txt')  # 전사 파일 저장 경로\n",
    "\n",
    "            # 오디오 데이터 저장\n",
    "            audio = example['audio']\n",
    "            sf.write(audio_path, audio['array'], audio['sampling_rate'])\n",
    "\n",
    "            # 전사 데이터 저장\n",
    "            with open(transcription_path, 'w') as transcription_file:\n",
    "                transcription_file.write(example['transcription'])\n",
    "\n",
    "            print(f'{i} done')\n",
    "            i += 1\n",
    "\n",
    "    # 다음 반복을 위해 시작 인덱스 업데이트\n",
    "    start_index += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "31eb2037-eed8-440b-abe1-4b9b11544d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "i = 1\n",
    "\n",
    "result_txt = \"\"\n",
    "paimon_folder = 'datasets/wavs'\n",
    "\n",
    "while True:\n",
    "    tmp_txt = \"\"\n",
    "\n",
    "    audio_path = f'{i}_audio'  # 오디오 파일 저장 경로\n",
    "    transcription_path = os.path.join(paimon_folder, f'{i}_transcription.txt')  # 전사 파일 저장 경로\n",
    "    try:\n",
    "        with open(transcription_path, 'r') as transcription_file:\n",
    "            transcription = transcription_file.read()\n",
    "\n",
    "        tmp_txt = f\"{audio_path},{transcription}\\n\"\n",
    "        result_txt += tmp_txt\n",
    "        i += 1\n",
    "    except:\n",
    "        print(\"done\")\n",
    "        break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "62538b8d-db50-4899-be65-7e2823e4afb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./datasets/metadata.csv\", 'w') as file:\n",
    "    file.write(result_txt)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c1d21eb5-0c7a-4aa4-84b1-0c3418c94a53",
   "metadata": {},
   "source": [
    "여기서부터는 finetuning\n",
    "TTS finetuning\n",
    "https://docs.coqui.ai/en/latest/finetuning.html\n",
    "\n",
    "XTTS\n",
    "https://www.gpters.org/c/ai-developers/2c022a\n",
    "\n",
    "/home/pengejeen/paymon_v0.0.1/paimon_llama2-enhanced/tts_paimon/TTS/recipes/ljspeech/xtts_v2\n",
    "\n",
    "tokenizer.encode\n",
    "https://github.com/coqui-ai/TTS/issues/3356"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5865231b-b624-4ada-ad22-33aeaa8219be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> DVAE weights restored from: /home/pengejeen/paymon_v0.0.1/paimon_llama2-enhanced/tts_paimon/TTS/recipes/ljspeech/xtts_v2/run/training/XTTS_v2.0_original_model_files/dvae.pth\n",
      " | > Found 386 files in /home/pengejeen/paymon_v0.0.1/paimon_llama2-enhanced/tts_paimon/datasets\n",
      " > Training Environment:\n",
      " | > Backend: Torch\n",
      " | > Mixed precision: False\n",
      " | > Precision: float32\n",
      " | > Current device: 0\n",
      " | > Num. of GPUs: 1\n",
      " | > Num. of CPUs: 12\n",
      " | > Num. of Torch Threads: 1\n",
      " | > Torch seed: 1\n",
      " | > Torch CUDNN: True\n",
      " | > Torch CUDNN deterministic: False\n",
      " | > Torch CUDNN benchmark: False\n",
      " | > Torch TF32 MatMul: False\n",
      " > Start Tensorboard: tensorboard --logdir=/home/pengejeen/paymon_v0.0.1/paimon_llama2-enhanced/tts_paimon/TTS/recipes/ljspeech/xtts_v2/run/training/paimon_ko-May-25-2024_02+17AM-b772972\n",
      "\n",
      " > Model has 518442047 parameters\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 0/6\u001b[0m\n",
      " --> /home/pengejeen/paymon_v0.0.1/paimon_llama2-enhanced/tts_paimon/TTS/recipes/ljspeech/xtts_v2/run/training/paimon_ko-May-25-2024_02+17AM-b772972\n",
      " > Filtering invalid eval samples!!\n",
      " > Total eval samples after filtering: 3\n",
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n",
      " | > Synthesizing test sentences.\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time: 0.3885319232940674 \u001b[0m(+0)\n",
      "     | > avg_loss_text_ce: 0.0338304303586483 \u001b[0m(+0)\n",
      "     | > avg_loss_mel_ce: 3.4577364921569824 \u001b[0m(+0)\n",
      "     | > avg_loss: 3.4915668964385986 \u001b[0m(+0)\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 1/6\u001b[0m\n",
      " --> /home/pengejeen/paymon_v0.0.1/paimon_llama2-enhanced/tts_paimon/TTS/recipes/ljspeech/xtts_v2/run/training/paimon_ko-May-25-2024_02+17AM-b772972\n",
      " > Sampling by language: dict_keys(['ko'])\n",
      "\n",
      "\u001b[1m > TRAINING (2024-05-25 02:18:05) \u001b[0m\n",
      "\n",
      "\u001b[1m   --> TIME: 2024-05-25 02:18:05 -- STEP: 0/128 -- GLOBAL_STEP: 0\u001b[0m\n",
      "     | > loss_text_ce: 0.036824099719524384  (0.036824099719524384)\n",
      "     | > loss_mel_ce: 3.9633591175079346  (3.9633591175079346)\n",
      "     | > loss: 0.0476212278008461  (0.0476212278008461)\n",
      "     | > current_lr: 3e-06 \n",
      "     | > step_time: 0.2831  (0.2831137180328369)\n",
      "     | > loader_time: 0.3716  (0.3715627193450928)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2024-05-25 02:20:33 -- STEP: 50/128 -- GLOBAL_STEP: 50\u001b[0m\n",
      "     | > loss_text_ce: 0.0333406999707222  (0.03540178839117289)\n",
      "     | > loss_mel_ce: 3.8040740489959717  (3.6619903373718263)\n",
      "     | > loss: 0.04568351060152054  (0.044016573503613474)\n",
      "     | > current_lr: 3e-06 \n",
      "     | > step_time: 1.2183  (2.4407589912414545)\n",
      "     | > loader_time: 0.0131  (0.011626739501953125)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2024-05-25 02:23:09 -- STEP: 100/128 -- GLOBAL_STEP: 100\u001b[0m\n",
      "     | > loss_text_ce: 0.035176414996385574  (0.03540576815605163)\n",
      "     | > loss_mel_ce: 3.9138667583465576  (3.6349197149276735)\n",
      "     | > loss: 0.04701242223381996  (0.04369435161352157)\n",
      "     | > current_lr: 3e-06 \n",
      "     | > step_time: 2.2118  (2.4288569188117966)\n",
      "     | > loader_time: 0.0202  (0.01185560941696167)\n",
      "\n",
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n",
      " | > Synthesizing test sentences.\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.016506195068359375 \u001b[0m(-0.372025728225708)\n",
      "     | > avg_loss_text_ce:\u001b[92m 0.03381115198135376 \u001b[0m(-1.9278377294540405e-05)\n",
      "     | > avg_loss_mel_ce:\u001b[92m 3.307185411453247 \u001b[0m(-0.15055108070373535)\n",
      "     | > avg_loss:\u001b[92m 3.340996503829956 \u001b[0m(-0.15057039260864258)\n",
      "\n",
      " > BEST MODEL : /home/pengejeen/paymon_v0.0.1/paimon_llama2-enhanced/tts_paimon/TTS/recipes/ljspeech/xtts_v2/run/training/paimon_ko-May-25-2024_02+17AM-b772972/best_model_128.pth\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 2/6\u001b[0m\n",
      " --> /home/pengejeen/paymon_v0.0.1/paimon_llama2-enhanced/tts_paimon/TTS/recipes/ljspeech/xtts_v2/run/training/paimon_ko-May-25-2024_02+17AM-b772972\n",
      "\n",
      "\u001b[1m > TRAINING (2024-05-25 02:25:22) \u001b[0m\n",
      "\n",
      "\u001b[1m   --> TIME: 2024-05-25 02:26:58 -- STEP: 22/128 -- GLOBAL_STEP: 150\u001b[0m\n",
      "     | > loss_text_ce: 0.038614239543676376  (0.03524735205891457)\n",
      "     | > loss_mel_ce: 4.668325424194336  (3.5998172868381846)\n",
      "     | > loss: 0.05603499710559845  (0.04327457956969738)\n",
      "     | > current_lr: 3e-06 \n",
      "     | > step_time: 2.6893  (2.8824721141295)\n",
      "     | > loader_time: 0.0066  (0.012252785942771217)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2024-05-25 02:30:17 -- STEP: 72/128 -- GLOBAL_STEP: 200\u001b[0m\n",
      "     | > loss_text_ce: 0.038584474474191666  (0.035195754866840095)\n",
      "     | > loss_mel_ce: 3.721092939376831  (3.547329157590866)\n",
      "     | > loss: 0.04475806653499603  (0.042649106639954776)\n",
      "     | > current_lr: 3e-06 \n",
      "     | > step_time: 1.8002  (2.740392847193611)\n",
      "     | > loader_time: 0.021  (0.011426024966769747)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2024-05-25 02:35:50 -- STEP: 122/128 -- GLOBAL_STEP: 250\u001b[0m\n",
      "     | > loss_text_ce: 0.03498125448822975  (0.03471009112650253)\n",
      "     | > loss_mel_ce: 3.036252975463867  (3.516279912385784)\n",
      "     | > loss: 0.03656231239438057  (0.042273691168329755)\n",
      "     | > current_lr: 3e-06 \n",
      "     | > step_time: 4.7483  (3.405438382117474)\n",
      "     | > loader_time: 0.021  (0.010712471164640835)\n",
      "\n",
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n",
      " | > Synthesizing test sentences.\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[91m 0.030072689056396484 \u001b[0m(+0.01356649398803711)\n",
      "     | > avg_loss_text_ce:\u001b[92m 0.03378177434206009 \u001b[0m(-2.9377639293670654e-05)\n",
      "     | > avg_loss_mel_ce:\u001b[92m 3.2006754875183105 \u001b[0m(-0.10650992393493652)\n",
      "     | > avg_loss:\u001b[92m 3.23445725440979 \u001b[0m(-0.10653924942016602)\n",
      "\n",
      " > BEST MODEL : /home/pengejeen/paymon_v0.0.1/paimon_llama2-enhanced/tts_paimon/TTS/recipes/ljspeech/xtts_v2/run/training/paimon_ko-May-25-2024_02+17AM-b772972/best_model_256.pth\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 3/6\u001b[0m\n",
      " --> /home/pengejeen/paymon_v0.0.1/paimon_llama2-enhanced/tts_paimon/TTS/recipes/ljspeech/xtts_v2/run/training/paimon_ko-May-25-2024_02+17AM-b772972\n",
      "\n",
      "\u001b[1m > TRAINING (2024-05-25 02:36:38) \u001b[0m\n",
      "\n",
      "\u001b[1m   --> TIME: 2024-05-25 02:40:13 -- STEP: 44/128 -- GLOBAL_STEP: 300\u001b[0m\n",
      "     | > loss_text_ce: 0.03148170933127403  (0.035770574838600376)\n",
      "     | > loss_mel_ce: 3.618353843688965  (3.456840834834359)\n",
      "     | > loss: 0.04345042631030083  (0.041578708216547966)\n",
      "     | > current_lr: 3e-06 \n",
      "     | > step_time: 3.9118  (3.406448234211315)\n",
      "     | > loader_time: 0.0064  (0.010734276338057085)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2024-05-25 02:44:25 -- STEP: 94/128 -- GLOBAL_STEP: 350\u001b[0m\n",
      "     | > loss_text_ce: 0.03386803716421127  (0.035086322139869335)\n",
      "     | > loss_mel_ce: 3.078688859939575  (3.4069926053919692)\n",
      "     | > loss: 0.03705424815416336  (0.040977131060146284)\n",
      "     | > current_lr: 3e-06 \n",
      "     | > step_time: 3.9319  (3.4105735991863493)\n",
      "     | > loader_time: 0.0057  (0.010290757138678368)\n",
      "\n",
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n",
      " | > Synthesizing test sentences.\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.022357702255249023 \u001b[0m(-0.007714986801147461)\n",
      "     | > avg_loss_text_ce:\u001b[92m 0.03375411778688431 \u001b[0m(-2.765655517578125e-05)\n",
      "     | > avg_loss_mel_ce:\u001b[92m 3.1191251277923584 \u001b[0m(-0.08155035972595215)\n",
      "     | > avg_loss:\u001b[92m 3.152879238128662 \u001b[0m(-0.08157801628112793)\n",
      "\n",
      " > BEST MODEL : /home/pengejeen/paymon_v0.0.1/paimon_llama2-enhanced/tts_paimon/TTS/recipes/ljspeech/xtts_v2/run/training/paimon_ko-May-25-2024_02+17AM-b772972/best_model_384.pth\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 4/6\u001b[0m\n",
      " --> /home/pengejeen/paymon_v0.0.1/paimon_llama2-enhanced/tts_paimon/TTS/recipes/ljspeech/xtts_v2/run/training/paimon_ko-May-25-2024_02+17AM-b772972\n",
      "\n",
      "\u001b[1m > TRAINING (2024-05-25 02:47:21) \u001b[0m\n",
      "\n",
      "\u001b[1m   --> TIME: 2024-05-25 02:48:25 -- STEP: 16/128 -- GLOBAL_STEP: 400\u001b[0m\n",
      "     | > loss_text_ce: 0.03013976290822029  (0.03535950847435744)\n",
      "     | > loss_mel_ce: 3.269782304763794  (3.2933138608932495)\n",
      "     | > loss: 0.03928478807210922  (0.03962706425227225)\n",
      "     | > current_lr: 3e-06 \n",
      "     | > step_time: 2.1608  (2.8341516852378845)\n",
      "     | > loader_time: 0.0058  (0.008571222424507141)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2024-05-25 02:52:07 -- STEP: 66/128 -- GLOBAL_STEP: 450\u001b[0m\n",
      "     | > loss_text_ce: 0.03214416280388832  (0.03407954760460241)\n",
      "     | > loss_mel_ce: 3.149967908859253  (3.313822298338919)\n",
      "     | > loss: 0.03788228705525398  (0.03985597498037599)\n",
      "     | > current_lr: 3e-06 \n",
      "     | > step_time: 3.3246  (3.1404427687327066)\n",
      "     | > loader_time: 0.0053  (0.007889953526583586)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2024-05-25 02:55:38 -- STEP: 116/128 -- GLOBAL_STEP: 500\u001b[0m\n",
      "     | > loss_text_ce: 0.03238408640027046  (0.034138330049684334)\n",
      "     | > loss_mel_ce: 3.2778728008270264  (3.3024455637767396)\n",
      "     | > loss: 0.039407823234796524  (0.039721237399197855)\n",
      "     | > current_lr: 3e-06 \n",
      "     | > step_time: 3.0481  (3.071147713167913)\n",
      "     | > loader_time: 0.0069  (0.007246923857721789)\n",
      "\n",
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n",
      " | > Synthesizing test sentences.\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.0194089412689209 \u001b[0m(-0.002948760986328125)\n",
      "     | > avg_loss_text_ce:\u001b[92m 0.0337030403316021 \u001b[0m(-5.1077455282211304e-05)\n",
      "     | > avg_loss_mel_ce:\u001b[92m 3.0701661109924316 \u001b[0m(-0.04895901679992676)\n",
      "     | > avg_loss:\u001b[92m 3.1038691997528076 \u001b[0m(-0.04901003837585449)\n",
      "\n",
      " > BEST MODEL : /home/pengejeen/paymon_v0.0.1/paimon_llama2-enhanced/tts_paimon/TTS/recipes/ljspeech/xtts_v2/run/training/paimon_ko-May-25-2024_02+17AM-b772972/best_model_512.pth\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 5/6\u001b[0m\n",
      " --> /home/pengejeen/paymon_v0.0.1/paimon_llama2-enhanced/tts_paimon/TTS/recipes/ljspeech/xtts_v2/run/training/paimon_ko-May-25-2024_02+17AM-b772972\n",
      "\n",
      "\u001b[1m > TRAINING (2024-05-25 02:56:47) \u001b[0m\n",
      "\n",
      "\u001b[1m   --> TIME: 2024-05-25 03:00:02 -- STEP: 38/128 -- GLOBAL_STEP: 550\u001b[0m\n",
      "     | > loss_text_ce: 0.034183111041784286  (0.034351689986100324)\n",
      "     | > loss_mel_ce: 3.163423538208008  (3.246376457967256)\n",
      "     | > loss: 0.038066744804382324  (0.03905628798039336)\n",
      "     | > current_lr: 3e-06 \n",
      "     | > step_time: 6.179  (3.038312874342266)\n",
      "     | > loader_time: 0.0068  (0.00867032377343429)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2024-05-25 03:04:14 -- STEP: 88/128 -- GLOBAL_STEP: 600\u001b[0m\n",
      "     | > loss_text_ce: 0.02937040477991104  (0.033872348489239805)\n",
      "     | > loss_mel_ce: 3.0373048782348633  (3.27630215883255)\n",
      "     | > loss: 0.03650803864002228  (0.03940684001215479)\n",
      "     | > current_lr: 3e-06 \n",
      "     | > step_time: 2.8655  (3.038136826320128)\n",
      "     | > loader_time: 0.0054  (0.008020972663706003)\n",
      "\n",
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n",
      " | > Synthesizing test sentences.\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[91m 0.020524978637695312 \u001b[0m(+0.001116037368774414)\n",
      "     | > avg_loss_text_ce:\u001b[92m 0.033644162118434906 \u001b[0m(-5.887821316719055e-05)\n",
      "     | > avg_loss_mel_ce:\u001b[92m 3.0384581089019775 \u001b[0m(-0.0317080020904541)\n",
      "     | > avg_loss:\u001b[92m 3.0721023082733154 \u001b[0m(-0.03176689147949219)\n",
      "\n",
      " > BEST MODEL : /home/pengejeen/paymon_v0.0.1/paimon_llama2-enhanced/tts_paimon/TTS/recipes/ljspeech/xtts_v2/run/training/paimon_ko-May-25-2024_02+17AM-b772972/best_model_640.pth\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=\"0\" python3 /home/pengejeen/paymon_v0.0.1/paimon_llama2-enhanced/tts_paimon/TTS/recipes/ljspeech/xtts_v2/train_gpt_xtts.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ced41c0-b76f-4bec-b3e2-390cff78af97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Xtts(\n",
       "  (gpt): GPT(\n",
       "    (conditioning_encoder): ConditioningEncoder(\n",
       "      (init): Conv1d(80, 1024, kernel_size=(1,), stride=(1,))\n",
       "      (attn): Sequential(\n",
       "        (0): AttentionBlock(\n",
       "          (norm): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
       "          (qkv): Conv1d(1024, 3072, kernel_size=(1,), stride=(1,))\n",
       "          (attention): QKVAttention()\n",
       "          (x_proj): Identity()\n",
       "          (proj_out): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): AttentionBlock(\n",
       "          (norm): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
       "          (qkv): Conv1d(1024, 3072, kernel_size=(1,), stride=(1,))\n",
       "          (attention): QKVAttention()\n",
       "          (x_proj): Identity()\n",
       "          (proj_out): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (2): AttentionBlock(\n",
       "          (norm): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
       "          (qkv): Conv1d(1024, 3072, kernel_size=(1,), stride=(1,))\n",
       "          (attention): QKVAttention()\n",
       "          (x_proj): Identity()\n",
       "          (proj_out): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (3): AttentionBlock(\n",
       "          (norm): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
       "          (qkv): Conv1d(1024, 3072, kernel_size=(1,), stride=(1,))\n",
       "          (attention): QKVAttention()\n",
       "          (x_proj): Identity()\n",
       "          (proj_out): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (4): AttentionBlock(\n",
       "          (norm): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
       "          (qkv): Conv1d(1024, 3072, kernel_size=(1,), stride=(1,))\n",
       "          (attention): QKVAttention()\n",
       "          (x_proj): Identity()\n",
       "          (proj_out): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (5): AttentionBlock(\n",
       "          (norm): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
       "          (qkv): Conv1d(1024, 3072, kernel_size=(1,), stride=(1,))\n",
       "          (attention): QKVAttention()\n",
       "          (x_proj): Identity()\n",
       "          (proj_out): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conditioning_dropout): Dropout1d(p=0.1, inplace=False)\n",
       "    (text_embedding): Embedding(6681, 1024)\n",
       "    (mel_embedding): Embedding(1026, 1024)\n",
       "    (gpt): GPT2Model(\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-29): 30 x GPT2Block(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (wte): Embedding(1026, 1024)\n",
       "    )\n",
       "    (mel_pos_embedding): LearnedPositionEmbeddings(\n",
       "      (emb): Embedding(608, 1024)\n",
       "    )\n",
       "    (text_pos_embedding): LearnedPositionEmbeddings(\n",
       "      (emb): Embedding(404, 1024)\n",
       "    )\n",
       "    (final_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (text_head): Linear(in_features=1024, out_features=6681, bias=True)\n",
       "    (mel_head): Linear(in_features=1024, out_features=1026, bias=True)\n",
       "    (conditioning_perceiver): PerceiverResampler(\n",
       "      (proj_context): Identity()\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x ModuleList(\n",
       "          (0): Attention(\n",
       "            (attend): Attend(\n",
       "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (to_q): Linear(in_features=1024, out_features=512, bias=False)\n",
       "            (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (to_out): Linear(in_features=512, out_features=1024, bias=False)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=5460, bias=True)\n",
       "            (1): GEGLU()\n",
       "            (2): Linear(in_features=2730, out_features=1024, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): RMSNorm()\n",
       "    )\n",
       "    (gpt_inference): GPT2InferenceModel(\n",
       "      (transformer): GPT2Model(\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-29): 30 x GPT2Block(\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2Attention(\n",
       "              (c_attn): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (wte): Embedding(1026, 1024)\n",
       "      )\n",
       "      (pos_embedding): LearnedPositionEmbeddings(\n",
       "        (emb): Embedding(608, 1024)\n",
       "      )\n",
       "      (embeddings): Embedding(1026, 1024)\n",
       "      (final_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (lm_head): Sequential(\n",
       "        (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): Linear(in_features=1024, out_features=1026, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (hifigan_decoder): HifiDecoder(\n",
       "    (waveform_decoder): HifiganGenerator(\n",
       "      (conv_pre): Conv1d(1024, 512, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "      (ups): ModuleList(\n",
       "        (0): ParametrizedConvTranspose1d(\n",
       "          512, 256, kernel_size=(16,), stride=(8,), padding=(4,)\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ParametrizedConvTranspose1d(\n",
       "          256, 128, kernel_size=(16,), stride=(8,), padding=(4,)\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): ParametrizedConvTranspose1d(\n",
       "          128, 64, kernel_size=(4,), stride=(2,), padding=(1,)\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): ParametrizedConvTranspose1d(\n",
       "          64, 32, kernel_size=(4,), stride=(2,), padding=(1,)\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (resblocks): ModuleList(\n",
       "        (0): ResBlock1(\n",
       "          (convs1): ModuleList(\n",
       "            (0): ParametrizedConv1d(\n",
       "              256, 256, kernel_size=(3,), stride=(1,), padding=(1,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): ParametrizedConv1d(\n",
       "              256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): ParametrizedConv1d(\n",
       "              256, 256, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (convs2): ModuleList(\n",
       "            (0-2): 3 x ParametrizedConv1d(\n",
       "              256, 256, kernel_size=(3,), stride=(1,), padding=(1,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ResBlock1(\n",
       "          (convs1): ModuleList(\n",
       "            (0): ParametrizedConv1d(\n",
       "              256, 256, kernel_size=(7,), stride=(1,), padding=(3,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): ParametrizedConv1d(\n",
       "              256, 256, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): ParametrizedConv1d(\n",
       "              256, 256, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (convs2): ModuleList(\n",
       "            (0-2): 3 x ParametrizedConv1d(\n",
       "              256, 256, kernel_size=(7,), stride=(1,), padding=(3,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): ResBlock1(\n",
       "          (convs1): ModuleList(\n",
       "            (0): ParametrizedConv1d(\n",
       "              256, 256, kernel_size=(11,), stride=(1,), padding=(5,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): ParametrizedConv1d(\n",
       "              256, 256, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): ParametrizedConv1d(\n",
       "              256, 256, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (convs2): ModuleList(\n",
       "            (0-2): 3 x ParametrizedConv1d(\n",
       "              256, 256, kernel_size=(11,), stride=(1,), padding=(5,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): ResBlock1(\n",
       "          (convs1): ModuleList(\n",
       "            (0): ParametrizedConv1d(\n",
       "              128, 128, kernel_size=(3,), stride=(1,), padding=(1,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): ParametrizedConv1d(\n",
       "              128, 128, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): ParametrizedConv1d(\n",
       "              128, 128, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (convs2): ModuleList(\n",
       "            (0-2): 3 x ParametrizedConv1d(\n",
       "              128, 128, kernel_size=(3,), stride=(1,), padding=(1,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): ResBlock1(\n",
       "          (convs1): ModuleList(\n",
       "            (0): ParametrizedConv1d(\n",
       "              128, 128, kernel_size=(7,), stride=(1,), padding=(3,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): ParametrizedConv1d(\n",
       "              128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): ParametrizedConv1d(\n",
       "              128, 128, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (convs2): ModuleList(\n",
       "            (0-2): 3 x ParametrizedConv1d(\n",
       "              128, 128, kernel_size=(7,), stride=(1,), padding=(3,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): ResBlock1(\n",
       "          (convs1): ModuleList(\n",
       "            (0): ParametrizedConv1d(\n",
       "              128, 128, kernel_size=(11,), stride=(1,), padding=(5,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): ParametrizedConv1d(\n",
       "              128, 128, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): ParametrizedConv1d(\n",
       "              128, 128, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (convs2): ModuleList(\n",
       "            (0-2): 3 x ParametrizedConv1d(\n",
       "              128, 128, kernel_size=(11,), stride=(1,), padding=(5,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): ResBlock1(\n",
       "          (convs1): ModuleList(\n",
       "            (0): ParametrizedConv1d(\n",
       "              64, 64, kernel_size=(3,), stride=(1,), padding=(1,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): ParametrizedConv1d(\n",
       "              64, 64, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): ParametrizedConv1d(\n",
       "              64, 64, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (convs2): ModuleList(\n",
       "            (0-2): 3 x ParametrizedConv1d(\n",
       "              64, 64, kernel_size=(3,), stride=(1,), padding=(1,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): ResBlock1(\n",
       "          (convs1): ModuleList(\n",
       "            (0): ParametrizedConv1d(\n",
       "              64, 64, kernel_size=(7,), stride=(1,), padding=(3,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): ParametrizedConv1d(\n",
       "              64, 64, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): ParametrizedConv1d(\n",
       "              64, 64, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (convs2): ModuleList(\n",
       "            (0-2): 3 x ParametrizedConv1d(\n",
       "              64, 64, kernel_size=(7,), stride=(1,), padding=(3,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): ResBlock1(\n",
       "          (convs1): ModuleList(\n",
       "            (0): ParametrizedConv1d(\n",
       "              64, 64, kernel_size=(11,), stride=(1,), padding=(5,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): ParametrizedConv1d(\n",
       "              64, 64, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): ParametrizedConv1d(\n",
       "              64, 64, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (convs2): ModuleList(\n",
       "            (0-2): 3 x ParametrizedConv1d(\n",
       "              64, 64, kernel_size=(11,), stride=(1,), padding=(5,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): ResBlock1(\n",
       "          (convs1): ModuleList(\n",
       "            (0): ParametrizedConv1d(\n",
       "              32, 32, kernel_size=(3,), stride=(1,), padding=(1,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): ParametrizedConv1d(\n",
       "              32, 32, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): ParametrizedConv1d(\n",
       "              32, 32, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (convs2): ModuleList(\n",
       "            (0-2): 3 x ParametrizedConv1d(\n",
       "              32, 32, kernel_size=(3,), stride=(1,), padding=(1,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): ResBlock1(\n",
       "          (convs1): ModuleList(\n",
       "            (0): ParametrizedConv1d(\n",
       "              32, 32, kernel_size=(7,), stride=(1,), padding=(3,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): ParametrizedConv1d(\n",
       "              32, 32, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): ParametrizedConv1d(\n",
       "              32, 32, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (convs2): ModuleList(\n",
       "            (0-2): 3 x ParametrizedConv1d(\n",
       "              32, 32, kernel_size=(7,), stride=(1,), padding=(3,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): ResBlock1(\n",
       "          (convs1): ModuleList(\n",
       "            (0): ParametrizedConv1d(\n",
       "              32, 32, kernel_size=(11,), stride=(1,), padding=(5,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): ParametrizedConv1d(\n",
       "              32, 32, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): ParametrizedConv1d(\n",
       "              32, 32, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (convs2): ModuleList(\n",
       "            (0-2): 3 x ParametrizedConv1d(\n",
       "              32, 32, kernel_size=(11,), stride=(1,), padding=(5,)\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conv_post): Conv1d(32, 1, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
       "      (cond_layer): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "      (conds): ModuleList(\n",
       "        (0): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
       "        (1): Conv1d(512, 128, kernel_size=(1,), stride=(1,))\n",
       "        (2): Conv1d(512, 64, kernel_size=(1,), stride=(1,))\n",
       "        (3): Conv1d(512, 32, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (speaker_encoder): ResNetSpeakerEncoder(\n",
       "      (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (layer1): Sequential(\n",
       "        (0): SEBasicBlock(\n",
       "          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): SELayer(\n",
       "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc): Sequential(\n",
       "              (0): Linear(in_features=32, out_features=4, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=4, out_features=32, bias=True)\n",
       "              (3): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): SEBasicBlock(\n",
       "          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): SELayer(\n",
       "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc): Sequential(\n",
       "              (0): Linear(in_features=32, out_features=4, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=4, out_features=32, bias=True)\n",
       "              (3): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): SEBasicBlock(\n",
       "          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): SELayer(\n",
       "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc): Sequential(\n",
       "              (0): Linear(in_features=32, out_features=4, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=4, out_features=32, bias=True)\n",
       "              (3): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): SEBasicBlock(\n",
       "          (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): SELayer(\n",
       "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc): Sequential(\n",
       "              (0): Linear(in_features=64, out_features=8, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=8, out_features=64, bias=True)\n",
       "              (3): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): SEBasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): SELayer(\n",
       "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc): Sequential(\n",
       "              (0): Linear(in_features=64, out_features=8, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=8, out_features=64, bias=True)\n",
       "              (3): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): SEBasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): SELayer(\n",
       "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc): Sequential(\n",
       "              (0): Linear(in_features=64, out_features=8, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=8, out_features=64, bias=True)\n",
       "              (3): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): SEBasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): SELayer(\n",
       "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc): Sequential(\n",
       "              (0): Linear(in_features=64, out_features=8, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=8, out_features=64, bias=True)\n",
       "              (3): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): SEBasicBlock(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): SELayer(\n",
       "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=16, out_features=128, bias=True)\n",
       "              (3): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): SEBasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): SELayer(\n",
       "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=16, out_features=128, bias=True)\n",
       "              (3): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): SEBasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): SELayer(\n",
       "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=16, out_features=128, bias=True)\n",
       "              (3): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): SEBasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): SELayer(\n",
       "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=16, out_features=128, bias=True)\n",
       "              (3): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): SEBasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): SELayer(\n",
       "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=16, out_features=128, bias=True)\n",
       "              (3): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): SEBasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): SELayer(\n",
       "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=16, out_features=128, bias=True)\n",
       "              (3): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): SEBasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): SELayer(\n",
       "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=32, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=32, out_features=256, bias=True)\n",
       "              (3): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): SEBasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): SELayer(\n",
       "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=32, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=32, out_features=256, bias=True)\n",
       "              (3): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): SEBasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): SELayer(\n",
       "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=32, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=32, out_features=256, bias=True)\n",
       "              (3): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (instancenorm): InstanceNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (torch_spec): Sequential(\n",
       "        (0): PreEmphasis()\n",
       "        (1): MelSpectrogram(\n",
       "          (spectrogram): Spectrogram()\n",
       "          (mel_scale): MelScale()\n",
       "        )\n",
       "      )\n",
       "      (attention): Sequential(\n",
       "        (0): Conv1d(2048, 128, kernel_size=(1,), stride=(1,))\n",
       "        (1): ReLU()\n",
       "        (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Conv1d(128, 2048, kernel_size=(1,), stride=(1,))\n",
       "        (4): Softmax(dim=2)\n",
       "      )\n",
       "      (fc): Linear(in_features=4096, out_features=512, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from TTS.tts.configs.xtts_config import XttsConfig\n",
    "from TTS.tts.models.xtts import Xtts\n",
    "\n",
    "#if you learning new model, please update PATH\n",
    "PATH = \"/home/pengejeen/paymon_v0.0.1/paimon_llama2-enhanced/tts_paimon/TTS/recipes/ljspeech/xtts_v2/run/training/paimon_ko-May-25-2024_02+17AM-b772972\"\n",
    "\n",
    "TOKENIZER_PATH = \"/home/pengejeen/paymon_v0.0.1/paimon_llama2-enhanced/tts_paimon/TTS/recipes/ljspeech/xtts_v2/run/training/XTTS_v2.0_original_model_files/vocab.json\"\n",
    "\n",
    "print(\"Loading model...\")\n",
    "config = XttsConfig()\n",
    "config.load_json(f\"{PATH}/config.json\")\n",
    "\n",
    "model = Xtts.init_from_config(config)\n",
    "model.load_checkpoint(\n",
    "    config,\n",
    "    checkpoint_dir = f\"{PATH}\",\n",
    "    vocab_path = TOKENIZER_PATH,\n",
    "    use_deepspeed=False  \n",
    ")\n",
    "model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cee213d4-4b3c-43ad-8ea8-401fdff00cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing speaker latents...\n",
      "Inference...\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing speaker latents...\")\n",
    "gpt_cond_latent, speaker_embedding = model.get_conditioning_latents(\n",
    "    audio_path=[\"./datasets/wavs/2_audio.wav\"]\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Inference...\")\n",
    "out = model.inference(\n",
    "    \"이거 모델 왜 지랄났냐 똑바로 안되냐!\",\n",
    "    \"ko\",\n",
    "    gpt_cond_latent,\n",
    "    speaker_embedding,\n",
    "    temperature=0.7\n",
    ")\n",
    "torchaudio.save(\"./output.wav\", torch.tensor(out[\"wav\"]).unsqueeze(0), 24000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29723cfe-4dfb-464f-ac87-d3b8fad15ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.16.2 at http://localhost:6007/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=/home/pengejeen/paymon_v0.0.1/paimon_llama2-enhanced/tts_paimon/TTS/recipes/ljspeech/xtts_v2/run/training/paimon_ko-May-25-2024_02+17AM-b772972"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b17ecb95-d5c2-46bc-8903-94785f154889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.16.2 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=/home/pengejeen/paymon_v0.0.1/paimon_llama2-enhanced/tts_paimon/TTS/recipes/ljspeech/xtts_v2/run/training/paimon_ko-May-24-2024_11+54PM-b772972"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "dc18ba65-d543-46e7-8d62-95e7439087ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > tts_models/multilingual/multi-dataset/xtts_v2 is already downloaded.\n",
      " > Using model: xtts\n",
      " > Text: 시발\n",
      " > Text splitted to sentences.\n",
      "['시발']\n",
      " > Processing time: 0.9812474250793457\n",
      " > Real-time factor: 0.5280287417756632\n",
      " > Saving output to tts_output.wav\n"
     ]
    }
   ],
   "source": [
    " !tts --model_name tts_models/multilingual/multi-dataset/xtts_v2 \\\n",
    "     --text \"시발\" \\\n",
    "     --speaker_idx \"Ana Florence\" \\\n",
    "     --language_idx ko \\\n",
    "     --use_cuda true"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
